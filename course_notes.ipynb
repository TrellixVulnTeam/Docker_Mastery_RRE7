{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is docker? The three innovations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD / SHIP / RUN\n",
    "The **BUILD/SHIP/RUN** cycle consists of three steps to take previously non containarized software, turn it into a docker image, put it on a registry and deploy it on containers.\n",
    "\n",
    "Most projects that gradudated from [the cloud native foundation](https://cncf.io) make the assumption that you're using a container workflow of build/ship/run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The three innovations\n",
    "1. The docker **IMAGE**\n",
    "2. The docker **REGISTRY**\n",
    "3. The docker **CONTAINER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The docker registry \n",
    "\n",
    "* Known as the OCI registry spec, a standard for distributing the images.\n",
    "* Docker Hub is the default, github and gitlab have their own.\n",
    "* A built image has a unique SHA hash that will allow or verification between two systems/hosts that the image contains the same files/metadata...\n",
    "* Usualy done via the command docker push or docker pull\n",
    "* **This is one of the core values of docker. We can take software that made on one machine with all its dependencies and run it on another machine, seamlessly and regardless if the two machines are running on different distributions of an OS (will run the same on Ubuntu or CentOs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The docker container \n",
    "\n",
    "* Running an image after downloading it into a server.\n",
    "* A container is known as a **namespace**. This is what makes sure that the app running inside the containers can't see the rest of the operating system.\n",
    "    * A namespace is basicly a blanch file system with only the files from the image that was built.\n",
    "    * It would have it's own IP address, NIC (Network Interface Controller) and process list.\n",
    "* Reruning the docker run command on that image would spin up a new instance of this image that is isolated from the first.\n",
    "    * If the docker run command if run on another server with a docker engine then it yields a high availability setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Container Run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [Play With Docker Website](https://labs.play-with-docker.com/) we can run a few demo commands before doing any local installs of Docker\n",
    "\n",
    "* You will need an account with [Docker](https://hub.docker.com/), it's free\n",
    "* Once logged in to [Play With Docker Website](https://labs.play-with-docker.com/) you will see a green start button and once clicked a timer will be show for the time remaining in the session.\n",
    "* You will be able to deploy docker images on infrastructure provided by Docker.\n",
    "* Steps for demo:\n",
    "    * click on 'add new instances'\n",
    "    * run \"docker version\" :\n",
    "        * Gets details about the client and the engine currently running\n",
    "    * connecting to from client to server can happen over many ways:\n",
    "        * Socket\n",
    "        * TCP/TLS\n",
    "        * SSH tunnel\n",
    "    * run \"docker run -d -p 8800:80 httpd\"\n",
    "        * -d -> detach, runs in background\n",
    "        * -p -> port, which ports are opened and published on host IP so that it can be accessed remotely\n",
    "            * first number is the port on the host on which we're listening\n",
    "            * second number is the container port on which we're connecting\n",
    "        * httpd -> apache server image\n",
    "        * ouput will resemble the below:\n",
    "            * Unable to find image 'httpd:latest' locally\n",
    "            * latest: Pulling from library/httpd\n",
    "            * 1efc276f4ff9: Pull complete \n",
    "            * aed046121ed8: Pull complete \n",
    "            * 4340e7be3d7f: Pull complete \n",
    "            * 80e368ef21fc: Pull complete \n",
    "            * 80cb79a80bbe: Pull complete \n",
    "            * Digest: sha256:343452ec820a5d59eb3ab9aaa6201d193f91c3354f8c4f29705796d9353d4cc6\n",
    "            * Status: Downloaded newer image for httpd:latest\n",
    "            * bbaf9c895f19f301e40c7d2bd743127e372f8932b997ca238b5eaaeb13f6ec88\n",
    "        * The output can be interpreted as such:\n",
    "            * the various **layers** of the image are downloaded/pull\n",
    "            * then it will create the networking, virtual interface, file system, load the file from the image into the file system\n",
    "            * then it will startup the httpd process in its own namespace.  \n",
    "    * run a curl command on the local host (machine from which this container is being instanciated) to verify that it's up: curl localhost:8800\n",
    "        * output is: ```<html><body><h1>It works!</h1></body></html >```\n",
    "    * run \"docker ps\" to view running containers:\n",
    "        * | CONTAINER ID | IMAGE | COMMAND            | CREATED        | STATUS        | PORTS                | NAMES             |\n",
    "          |--------------|-------|--------------------|----------------|---------------|----------------------|-------------------|\n",
    "          | bbaf9c895f19 | httpd | \"httpd-foreground\" | 12 minutes ago | Up 12 minutes | 0.0.0.0:8800->80/tcp | confident_johnson |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Docker? Why Now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker's \"raison d'etre\" come from frictions that were being faced relating to speed a efficiency of packaging and deploying software.\n",
    "\n",
    "The three main reason are:\n",
    "* Isolation:\n",
    "    * During the move from mainframe to servers (early 2000s), it was very rare that an app would fully utilise the capacity of the servers on which they would run. You ended up with mutliple apps colocating on the same servers, this increased complexity in managing them and made them prone to outages (e.g: one change for an app would cause an effect on other apps).\n",
    "    * This lead to the emergence of VMs (Virtual Machines), fewer apps on multiple VMs all riding on the same hosts. This would lead to an exploding number of VMs which were each spun up with their own OS. Isolation was sort of solved but complexity outweighed the benefits.\n",
    "    * Then Docker comes along and removes the need for all the different operating systems on each VM. Each application has its own namespace that is isolated and managing them comes easily through the docker/kubernetes cli. You can now run multiple version of the same app on the same host without any conflict.\n",
    "* Environment\n",
    "    * With VMs also came the deployement of a large number of types of environments. The running joke being \"Works on my machine\".\n",
    "    * The container image has become a sort of contract between what the app runs and where its running. Just like actual shipping containers where those who ship something don't care what the ship that was carrying the container looked like and the people transporting the container didn't care what was inside it.\n",
    "        * This is done through a consistent standard (the OCI format) which has two specs, an image and a runtime spec.\n",
    "        * this ensures that the container will always be run with the same consistent way and same dependencies it was built with.\n",
    "* Speed\n",
    "    * Not the speed of CPU and processors but the speed of business, the ability to test and execute on ideas in a timely manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Course Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Course Ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github repo\n",
    "https://github.com/bretfisher/udemy-docker-mastery\n",
    "\n",
    "### Docker commands, cheatsheets & slides\n",
    "Downloaded in the \"course_ressources folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: The best way to setup Docker for your OS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Docker\n",
    "\n",
    "Get the [Docker Desktop](https://www.docker.com/products/docker-desktop/) and install it on your machine.\n",
    "\n",
    "There are 3 major ways to run containers:\n",
    "* Locally (Docker Desktop, Remote Desktop)\n",
    "* Servers (Docker Engine, K8s)\n",
    "* PaaS (Cloud Run, Fargate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Creating and usig containers like a boss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.Check the Docker Install and Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the current version of Docker\n",
    "Returns the version of the **client** as well the server, a.k.a **the engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get additional information on Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the full list of commands in Docker\n",
    "\n",
    "This is not a full list of the commands. You will notice below the sub modules that are accessible via docker manage  \n",
    "commands, under **Management Commands:**\n",
    "  \n",
    "Management Commands:\n",
    "* builder     Manage builds\n",
    "* buildx*     Docker Buildx (Docker Inc., v0.8.2)\n",
    ".  \n",
    ".  \n",
    ".  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.Starting a Nginx web server\n",
    "\n",
    "In this section we will:\n",
    "* contract images to containers\n",
    "* run/stop/remote containers\n",
    "* check container logs and process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images vs. Containers\n",
    "\n",
    "**Images** are the binaries/libraries/source code that will make up our application. A **container** is a running   \n",
    "instance of that image. You can have mutliple containers running from the same image. \n",
    "\n",
    "Images are obtained from registries, [**Docker Hub**](https://hub.docker.com/) being the default registry.   \n",
    "(sort of what github is to source code, registries are to images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# command to get the image from the registry and running it\n",
    "docker container run --publish 80:80 nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the background docker looked for the latest version of nginx in Docker Hub,  \n",
    "pulled it into our machine and started it  \n",
    "as a process. The --publish argument exposes my local port 80 and sends all  \n",
    "traffic comming to it to the container's  \n",
    "port 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want the process to run in the foreground we can add the --detach  \n",
    "argument and have all the above be done in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container run --publish 80:80 --detach nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the container was started in the background we got in the output the unique container ID which  \n",
    "we can use to compare with the output of the ```docker ps``` or ```docker container ls``` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# we can check that the container is actually up and running\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the running container you can use ```docker container stop ${CONTAINER_ID}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container stop 188eb98e7d4e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify that the container is no longer running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If i want to see all the containers, even those that are not running then i use   \n",
    " ```docker container ls -a```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always give names to our containers otherwise they will be given randomly generated  \n",
    "names using this syntax `<adjective>_<name_of_famous_scientist>`.   \n",
    "\n",
    "If we want to specify the name we use the argument `--name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container run --publish 80:80 --detach --name webhost nginx\n",
    "docker container ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to view the container logs we use `docker container logs ${CONTAINER_NAME}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container logs webhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the processes in the container use `docker container top ${CONTAINER_NAME}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container top webhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#you can see that the new container had a custom name\n",
    "docker container stop d4ae91d827ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup the containers by listing all containers and removing them using  \n",
    "`docker container rm [${CONTAINER_ID}  ${CONTAINER_ID} ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm d4a 188\n",
    "docker container ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.Debrief, what happens when we run a container\n",
    "\n",
    "1. Looks for images in local cache\n",
    "2. Look for images in registry if not found locally\n",
    "3. Download the latest version\n",
    "4. Create and new container based on the image and prepare it for start-up\n",
    "5. Give the container a virtual IP on private network within the docker engine\n",
    "6. Open up a port on the host and forward it to the port in the container\n",
    "7. Start container based on CMD found in the image's Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.Containers vs Virtual Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers aren't mini VMs\n",
    "\n",
    "* They are just processes\n",
    "* They are limited to what ressources they can access\n",
    "* They exit when process stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.Window Containers: Should you consider them?\n",
    "\n",
    "Containers are restricted processes that are running on the host's OS kernel.  \n",
    "For example a Python image built for linux/x84_64 won't run as a python.exe on   \n",
    "a Windows kernel. Which is why Docker utilizes a lightwheight Linux VM to run  \n",
    "the containers.\n",
    "\n",
    "From Docker's inception in 2013 to 2016, we could build images for multiple  \n",
    "architectures (amd64, arm/v6, arm/v7, i386, etc.) but not for the Windows   \n",
    "kernel itself. Docker was Linux-only.\n",
    "\n",
    "In 2016 we got \"Windows Containers\" support from Microsoft. When you think of   \n",
    "images, realize they are always kernel (Linux/Windows) and architecture   \n",
    "(arm64, amd64, etc.) specific. Docker does this seamlessly in the background  \n",
    "with a \"manifest\". It downloads the best image for the platform your Docker  \n",
    "Engine is running on.\n",
    "\n",
    "You can enable Windows Container mode in Docker Desktop for Windows by clicking  \n",
    "\"Switch to Windows containers\" in the Docker whale menu, which then switches  \n",
    "from WSL2 to Hyper-V running a slim Windows VM. It's an either-or setting,  \n",
    "you'll have to decide which OS you want to run your containers on and stick to it.  \n",
    "\n",
    "Sadly, the truth is that Windows Containers never caught on in a major way.  \n",
    "Microsoft even built a Windows-based MSSQL image, but has since discontinued   \n",
    "it in 2021 in favor of their Linux-based MSSQL image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.Arm Support for MySQL\n",
    "\n",
    "The official MySQL image only supports Intel/AMD processors. MariaDB is an  \n",
    "alternative if you're using Apple M1 or Raspberry Pi (both based on arm64 processors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23-24.Assignement: Manage multiple containers\n",
    "\n",
    "[Docker Official Documentation](https://docs.docker.com) is your friend as well  as the --help argument at the end of each command.\n",
    "\n",
    "### Assignement Instructions\n",
    "* Run an `nginx`, `mysql` and `httpd` (apache) server\n",
    "* Run all of the in `--detach` mode (-d), name them with --name\n",
    "* Specify the ports, nginx should listen to 80:80, httpd to 8080:80 and mysql to 3306:3306\n",
    "* When running mysql, use the `--env` option (-e) to pass in `MYSQL_RANDOM_ROOT_PASSWORD=yes`\n",
    "* Use the `docker container logs` on mysql to find the random password on startup\n",
    "* Clean it all up afterwards using the `docker container stop` and `docker container rm`\n",
    "* Verify that everything got shutdown correctly using the `docker container ls`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# run instances\n",
    "docker container run --detach -p 80:80 --name nginx nginx\n",
    "docker container run --detach -p 8080:80 --name httpd httpd\n",
    "docker container run --detach -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD='yes' --name mysql mysql \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "#verify instances\n",
    "docker container ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# find generated password\n",
    "docker container logs mysql | grep \"GENERATED ROOT PASSWORD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cleanup\n",
    "docker container stop nginx\n",
    "docker container stop httpd\n",
    "docker container stop mysql\n",
    "docker container rm nginx\n",
    "docker container rm httpd\n",
    "docker container rm mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.What's going on in containers: CLI process monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands to view container processes\n",
    "\n",
    "1. docker cotainer top - process list in one container\n",
    "2. docker container inspect - detials of one container config\n",
    "3. docker container stats - performance stats for all containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# run instances\n",
    "docker container run --detach -p 80:80 --name nginx nginx\n",
    "docker container run --detach -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD='yes' --name mysql mysql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container top nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container inspect mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cleanup\n",
    "docker container stop nginx\n",
    "docker container stop mysql\n",
    "docker container rm nginx\n",
    "docker container rm mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26-27. Getting a shell inside the container\n",
    "\n",
    "### Useful commands\n",
    "\n",
    "1. docker container run -it - start a new container interactively\n",
    "2. docker container exec -it - run additional commands in existing container\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docker run -it\n",
    "-i : interactive  \n",
    "-t : allocate a pseudo TTY (a prompt similar to ssh)  \n",
    "bash : an optional argument that can be passed to the container on startup  \n",
    "here it makes the container start with a bash terminal  \n",
    "\n",
    "\n",
    "docker container run -it --name nginx nginx bash\n",
    "\n",
    "#### Command \n",
    "docker container run -it --name nginx nginx bash\n",
    "\n",
    "Notice that when exiting the shell the container gets stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docker run -it\n",
    "\n",
    "If I want to see a shell inside an already running container\n",
    "\n",
    "#### Command:\n",
    "docker container exec -it nginx bash\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-29 Docker networks: concepts for private and public comms in containers\n",
    "\n",
    "When starting a container you're connecting, in the background, to a particular  \n",
    "docker network (the default being the bridge network).\n",
    "\n",
    "\n",
    "Each virtual network routes through a NAT firewall on host IP. The Docker deamon  \n",
    "configuring the host IP on it default interface so that your containers can   \n",
    "interact with the internet.\n",
    "\n",
    "[NAT Firewal Definition](https://nordvpn.com/blog/what-is-nat-firewall/)\n",
    "```  \n",
    "A Network Address Translation (NAT) firewall operates on a router to protect  \n",
    "private networks. It works by only allowing internet traffic to pass through if  \n",
    "a device on the private network requested it. A NAT firewall protects the   \n",
    "identity of a network and doesn't show internal IP addresses to the internet.\n",
    "```  \n",
    "\n",
    "All containers within a network can talk to each other inside the host  \n",
    "without the -p (port). For example a mysql and a php/apache container can  \n",
    "communicate on the same network named \"my_web_app\" without exposing their ports  \n",
    "to the rest of the physical network.\n",
    "\n",
    "This brings us to the concept of `\"Batteries included, but removable\"` which  \n",
    "means that the defaults work well in most cases but are easily customizable  \n",
    "(creating mutliple network, one per app, or different networks depending on  \n",
    "security requirements)\n",
    "\n",
    "You can attach container to more than one virtual network (or none).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container run -d  -p 80:80 --name nginx nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Here we see that the host port 80 is exposed to the container's port 80 \n",
    "#through a tcp connexion\n",
    "docker container port nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#We may assume that the container uses the same IP as the host but by default \n",
    "#that is not true. We'll use the format comand instead of grep to filter\n",
    "#to the correct node in the json output off the docker inspect command\n",
    "docker container inspect --format \"{{.NetworkSettings.IPAddress}}\" nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker container stop nginx\n",
    "docker container rm nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#check the host's IP address, it does't have to match that of the host\n",
    "ifconfig en0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30.Docker Netwoks: CLI Management of Virtual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "\n",
    "* Show networks `docker network ls`\n",
    "* Inspect a network `docker network inspect`\n",
    "* Create a network `docker network create --driver`\n",
    "    * It has the optional `--driver` argument that we can use to specify built-in  \n",
    "    or third-party driver to create virtual networks\n",
    "* Attach/detach a network to a conttainer `docker netwrok connect/disconnect`\n",
    "    * Used t connect/disconnect a live running container so that a NIC is   \n",
    "    created/destroyed on a virtual network for that container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker network ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge Network \n",
    "\n",
    "The bridge network is the default network that bridges throught the [NAT firewall](https://www.comparitech.com/blog/vpn-privacy/nat-firewall/)  \n",
    "to the physical network that your host is connected to.\n",
    "\n",
    "* (Note) A NAT firewall works by only allowing internet traffic to pass through the gateway if a device on the private network requested it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker network inspect bridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## to find a specific item in the docker netork inspect\n",
    "docker network inspect bridge --format {{.IPAM.Config}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The host network\n",
    "\n",
    "A special kind of network that skips the virtual network of Docker and attaches  \n",
    "the container directly to the host interface. Its a boost for performance but  \n",
    "sacrifices in security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker network inspect host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new network\n",
    "\n",
    "Notice how the default for `DRIVER` for the new network was `bridge`. It's a  \n",
    "built-in driver that create a virtual network locally with its own subnet. It  \n",
    "lacks some of the more advanced features, such as private networking between hosts  \n",
    "like other 3rd party driver like `Weave`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker network create my_app_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker network ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#check out all the arguments that you can specify during the creatiion of networks\n",
    "docker network create --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning a network to a container on startup\n",
    "\n",
    "Using the `--network` option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container run -d -p 80:80 --name nginx --network my_app_net nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#You will see the new container running on this network\n",
    "docker network inspect my_app_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting running containers to existing networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker network --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker network ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## connect the nginx container (aready connected to the my_app_net) to the default  \n",
    "## bridge network\n",
    "docker network connect bridge nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "##inspecting the nginx container, you'll now see that its on two networks\n",
    "docker container inspect --format {{.NetworkSettings.Networks}} nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## to reverse out the above\n",
    "docker network disconnect bridge nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "##inspecting the nginx container, you'll now see only one left\n",
    "docker container inspect --format {{.NetworkSettings.Networks}} nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container stop nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container rm nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.Docker Networks: DNS & how containers find each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key concepts\n",
    "\n",
    "* Undertand how DNS is the key to inter-container comms\n",
    "    * You can't fully rely on IP adresses and need DNS because things are  \n",
    "    dynamic (i.e: if you stop a few containers but restart them in a different  \n",
    "    order, they may not have the same IP address)\n",
    "    * Docker uses the container names as the equivalent of host names when   \n",
    "    containers are talking to each other.\n",
    "* Understand how custom & default networks differ in how they deal with DNS\n",
    "* Learn how to use `--link` in the `container run` command to enable DNS on the   \n",
    "default bridge network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container run -d -p 80:80 --name nginx --network my_app_net nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic DNS resolution\n",
    "\n",
    "On the new netwrok **my_app_net** you get a special feature because its not the   \n",
    "default bridge network and that is `automatic DNS resolution` for all containers  \n",
    "on that network using their name. When a new container is created on that network  \n",
    "it will be able to communicate with other containers, on that network, regardless  \n",
    "of what the IP address is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker container run -d --name nginx_2 --network my_app_net nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker network inspect my_app_net --format {{.Containers}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# you can see from output that the two containers are able to communicate using  \n",
    "# only the container names\n",
    "docker container exec  -i nginx_2 ping nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcommings of the default bridge network\n",
    "\n",
    "The bridge network does not have any built-in DNS resolution. You can use `--link`  \n",
    "to create manual links between the containers as a workaround. However its much  \n",
    "easier to create a new network so as not to have to create these links every time.  \n",
    "Later when using `Docker Compose` it gets even easier as it will create these  \n",
    "virtual networks when you start up an entire app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32-33.Assignment: Using Containers for CLI Testing\n",
    "\n",
    "Scenario: You need to check the different versions of curl installed on two different  \n",
    "linux distros. In the VM era you'd have to wait for the OS to install and the VMs to  \n",
    "startup. Now all you have to do is spin up two containers and run your commands  \n",
    "from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Centos container\n",
    "\n",
    "    docker run --rm -it centos:7 bash\n",
    "\n",
    "**from within the container's shell**\n",
    "\n",
    "    yum update curl    \n",
    "    curl --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a ubuntu container\n",
    "\n",
    "    docker run --rm -it ubuntu:14.04 bash\n",
    "\n",
    "**from within the container's shell**\n",
    "\n",
    "    apt-get update && apt-get install curl\n",
    "    curl --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35-36.Assignment: DNS Robin Test\n",
    "\n",
    "DNS Round Robin is the concept of two hosts with DNS aliases that respond to the  \n",
    "same DNS name.\n",
    "\n",
    "[Wikipedia Round-robin DNS](https://en.wikipedia.org/wiki/Round-robin_DNS): \n",
    "\n",
    "In its simplest implementation, round-robin DNS works by responding to DNS requests  \n",
    "not only with a single potential IP address, but with a list of potential IP addresses   \n",
    "corresponding to several servers that host identical services.The order in   \n",
    "which IP addresses from the list are returned is the basis for the term round robin.   \n",
    "\n",
    "Since Docker Engine 1.11, we can have multiple containers on a network respond to  \n",
    "the ssame DNS address.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment steps:\n",
    "\n",
    "1. create a vitual network\n",
    "2. create two containers from the elasticsearch:2 image\n",
    "    * research and use `--network-alias search` when creating the containers to  \n",
    "    give them an addtional DNS name to responde to\n",
    "3. run alpine `nslookup search` with `--net` to see the two containers list for   \n",
    "the same DNS name\n",
    "4. run centos `curl -s search:9200` with `--net` multiple times untill you see both  \n",
    "name fields show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "##create network \n",
    "docker network create my_app_net\n",
    "\n",
    "##create two elasctisearch containers \n",
    "docker container run -d --name elasticsearch_1 --network my_app_net --network-alias search elasticsearch:2\n",
    "docker container run -d --name elasticsearch_2 --network my_app_net --network-alias search elasticsearch:2\n",
    "\n",
    "##create alpine and centos containers\n",
    "docker run -d -t --name alpine_1 --network my_app_net alpine\n",
    "docker run -dt --name centos_1 --network my_app_net centos:7\n",
    "\n",
    "##nslookup from within the alpine container\n",
    "docker exec alpine_1 nslookup search\n",
    "\n",
    "##curl from within the centos container\n",
    "docker exec centos_1 curl -s search:9200\n",
    "\n",
    "##cleanup \n",
    "docker container rm -f alpine_1\n",
    "docker container rm -f centos_1\n",
    "docker container rm -f elasticsearch_1\n",
    "docker container rm -f elasticsearch_2\n",
    "docker network rm my_app_net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Container images, where to find them and how to build them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37.What's In An Image (And What Isn't)\n",
    "\n",
    "What is an image:\n",
    "* An application's binaries and dependencies\n",
    "* The metadata about the image and how to run it\n",
    "\n",
    "**Official Definition**:  \n",
    "\"An image is an ordered collection of root filesystem changes and the   \n",
    "corresponding execution parameters for use within a container runtime.\"\n",
    "\n",
    "It is not a complete OS, it has no kernel/drivers etc. The host provides the  \n",
    "kernel. This is one of the main differences with virtual machines, a container  \n",
    "doesn't boot up a full operating system,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38.The Mighty Hub: Using Docker Hub Registry Images\n",
    "\n",
    "The official registry is found here: [Docker Hub](https://hub.docker.com/)\n",
    "\n",
    "Each application can have multiple images on the registry (e.g: nginx has   \n",
    "~90,000 images). To begin with, an easy choice is to start with the official   \n",
    "images that are curated by the team at Docker (who usualy work hand in hand   \n",
    "with the team that created the software).\n",
    "\n",
    "You can find documentation on how to run the image within its page on the hub.\n",
    "\n",
    "There are multiple versions of most official images. Each version can have more   \n",
    "than one tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39.Images & Their Layers: Discover The Image Cache\n",
    "\n",
    "Images don't just come as a single piece of data. They're usualy stacks of code   \n",
    "that build on top of each other (much like commits).\n",
    "\n",
    "They are designed using the union file concept of making layers of changes.\n",
    "\n",
    "The below is a history of the image layers. Every image start with a blank base   \n",
    "layer called \"scratch\", every change since then on the image file system is an   \n",
    "added layer. Every new layer has its own unique hash code (sha265).\n",
    "\n",
    "Images being stacks of layers can \"share\" cached layers that are common across   \n",
    "images (e.g: Two images can use the same scratch layer of Debian Jessie)\n",
    "\n",
    "In deploying containers, if you modify one of the files in the base layer that is   \n",
    "shared among containers. Only the difference is copied out. This is known as copy   \n",
    "on write, the differencing elements in the layers are copied into the container   \n",
    "that does the changes (not affecting the other containers). This means that the  \n",
    "containers can write files that will go on disk but the base image is read only  \n",
    "and will not be affected by what the containers do.\n",
    "\n",
    "The docker inspect command return metadata on the image in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## inspect the nginx history\n",
    "docker image history nginx:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## get the metadata on the image\n",
    "docker image inspect nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40.Image Tagging & Publishing to Docker Hub\n",
    "\n",
    "Images don't have names, they have `IMAGE IDs`, `TAGs`.\n",
    "\n",
    "The syntax for image tags is: \\<user>/\\<repo>:\\<tag>\n",
    "\n",
    "A tag is a pointer to a specific image commit.\n",
    "\n",
    "You can have multiple tag to a single image ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## you can retag existing docker images\n",
    "docker image tag nginx karimitn/nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## push your image into you own repo in the registry\n",
    "docker image push karimitn/nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41.Building Images: The Dockerfile Basics\n",
    "\n",
    "The Dockerfile is like a recipe for creating containers.\n",
    "\n",
    "You will find code arranged into sections called `Stanzas` . The Script will look  \n",
    "like a bash script but technically it is not (it's a language unique to Docker).  \n",
    "\n",
    "To run it you use the command `docker run -f <some-docker-file>`\n",
    "\n",
    "The first stanza is the `FROM` command, its mandatory and is usually a minimal  \n",
    "linux distribution.\n",
    "\n",
    "Next is the`ENV` stanza, they are the main way of setting keys and values to run  \n",
    "containers.\n",
    "\n",
    "NOTE: Each stanza is an individual layer of the docker image. So the order in which  \n",
    "they are setup is critical as they work top-down.\n",
    "\n",
    "`RUN` is used to run CLI commands in the container as its being built. The &&  \n",
    "in the stanza is used to bundle the commands together and make them into a single  \n",
    "layer.\n",
    "\n",
    "`EXPOSE`, by default no TCP or UDP port are open inside a container. They don't  \n",
    "expose anything to the virtual network unless they are listed in this stanza.  \n",
    "This however doesn't mean these ports are open automatically on our host, that's  \n",
    "what the `-p` flag is used for.\n",
    "\n",
    "`CMD` is arequired stanza that will be run everytime a new container is run from  \n",
    "from the image or restart a container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat ./course_ressources/udemy-docker-mastery/dockerfile-sample-1/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42.Building Images: Running Docker Builds\n",
    "\n",
    "So what do we do with a Dockerfile? It's barelt 3Mb in size so how will we be able to  \n",
    "use it for anything? \n",
    "\n",
    "The `FROM` stanza will pull the linux distro from docker hub into my local cache.  \n",
    "It will then execute each of those stanzas inside my docker engine and then cache  \n",
    "them as layers. Each layer once built has a cache that gets stored this way the next  \n",
    "time you want to build this image (or another image that uses the same layer)  \n",
    "we can do it much quicker.\n",
    "\n",
    "If you change any line in a stanza then that line will be rerun (not usuing the cached layer).  \n",
    "Every stanza following that will also be rerun not from the cache.\n",
    "\n",
    "For example if you changed the EXPOSE stanza (e.g: added port 8080) then the CMD  \n",
    "stanza will also be rerun not from cache on the next build. This means if you want  \n",
    "to keep your build times low then you should preferably put the layers that are least  \n",
    "likely to change in the future at the top of the stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -ltra ./course_ressources/udemy-docker-mastery/dockerfile-sample-1/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker image build -t custom_nginx -f ./course_ressources/udemy-docker-mastery/dockerfile-sample-1/Dockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43.Building Images: Extending Official Images\n",
    "\n",
    "`WORKDIR` is used to change the working directory  \n",
    "`COPY` is used to copy files from your local machine or build servers into the  \n",
    "the containers.  \n",
    "`CMD` command is mandatory but in this Dockerfile it is missing. However in the  \n",
    "`FROM` command we're using nginx, which has its own cmd specified in it. We inherit  \n",
    "from the images from which we're \"froming\". This is used to chain images and create  \n",
    "dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "ls -ltra ./course_ressources/udemy-docker-mastery/dockerfile-sample-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cat ./course_ressources/udemy-docker-mastery/dockerfile-sample-2/Dockerfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd ./course_ressources/udemy-docker-mastery/dockerfile-sample-2/\n",
    "docker image build -t nginx-with-html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# running the below and navigating to localhost:80 will display a page with this text:\n",
    "# You just successfully ran a container with a custom file copied into the image at build time!\n",
    "docker run -p 80:80 --rm --name nginx-with-html nginx-with-html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker rm -f nginx-with-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44-45.Assignment Build Your Own Dockerfile and Run Containers From It\n",
    "\n",
    "You don't necessarily need to understand the app that you're trying to \"Dockerize\".  \n",
    "You may however need to search and match images with what you need to build.\n",
    "\n",
    "In this assignement you're going to take an existing Node.js app and Dockerize it.  \n",
    "You don't have to know much about Node.js to be able to Dockerize it.\n",
    "\n",
    "Instruction for assignment are in   \n",
    "./course_ressources/udemy-docker-mastery/dockerfile-assignment-1/Dockerfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment Dockerfile :  \n",
    "\n",
    "FROM node:6-alpine\n",
    "\n",
    "EXPOSE 3000\n",
    "\n",
    "RUN apk add --update tini\n",
    "\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "COPY package.json package.json\n",
    "\n",
    "RUN npm install && npm cache clean --force\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD [ \"/sbin/tini\", \"--\", \"node\", \"./bin/www\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#after modifying the Dockerfile\n",
    "cd ./course_ressources/udemy-docker-mastery/dockerfile-assignment-1/\n",
    "docker build -t testnode ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker run --name testnode_1 -p 80:3000 testnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker rm -f testnode_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# tag and push image to your registry\n",
    "docker tag testnode karimitn/testnode\n",
    "\n",
    "docker push karimitn/testnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker image rm karimitn/testnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "#docker image rm nginx-with-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46.Usign Prune to Keep You Docker System Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker system df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker image prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker system prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6 : Container Lifetime and  Persistent Data: Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47.Container lifetime and persistent data\n",
    "\n",
    "Containers are designed to be imutable and ephemarale, so how do you persist data?  \n",
    "This bring us to the concept of `separation of concerns`, the container should only   \n",
    "contain the applications binaries and not the data that it generates.  \n",
    "\n",
    "Docker has two solutions:\n",
    "\n",
    "1. Data volumes: make special location outside of containers' UFS (Union File System)\n",
    "2. Bind mounts: link container path to host path\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48.Persistent Data: Data Volumes\n",
    "\n",
    "Let's checkout the [mysql image](https://hub.docker.com/_/mysql).  \n",
    "\n",
    "In the Dockerfile you will see a `VOLUME` command:  VOLUME /var/lib/mysql  \n",
    "\n",
    "This command tell the container that any file that we put in there will outlive  \n",
    "the containers untill we delete the volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker pull mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker image inspect mysql -f {{.ContainerConfig.Volumes}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run --name mysql -d -e MYSQL_ALOW_EMPTY_PASSWORD=True mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# we can see that the running container is gettig its own unique location to store  \n",
    "# data on the host. It thinks its writing to /var/lib/mysql but in actaullity it's \n",
    "# writing to the hosts /var/lib/docker/volumes/b62... directory\n",
    "docker container inspect mysql -f {{.Mounts}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker volume ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# in a linux machine you can just navigate to that mountpoint and see the data\n",
    "# however on Mac or Windows you can't because under teh hood the docker engine \n",
    "# is starting a linux VM in which the data lives\n",
    "docker volume inspect b62458d4f569966acd39a8db334d505198ae766869ea948eb568e13600ebe4cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named volumes\n",
    "\n",
    "The problem with volums is that from the `docker volume inspect` commnad you can't  \n",
    "see which containers are connected to the volume but from teh `docker container inspect`  \n",
    "you actually can.\n",
    "\n",
    "During the startup of a new container you can specify named containers to which   \n",
    "the containers will write. This is achieved using `-v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run --name mysql2 -d -e MYSQL_ALOW_EMPTY_PASSWORD=True  -v mysql-db:/var/lib/mysql mysql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker volume inspect mysql-db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48-49.Persisting Data: Bind Mounts\n",
    "\n",
    "A mapping of host file/directory into a container file/directory. They are not  \n",
    "specified in the Dockerfiles because they ar host specific.  \n",
    "\n",
    "`... run -v /User/Karim/stuff:/path/container`\n",
    "\n",
    "Docker knows that its a bind mount and not a volume because mounts are start with  \n",
    "a forward slash.\n",
    "\n",
    "What differenciates them from volumes is that when you change files from the host  \n",
    "these changes cann be seen in realtime inside the container which is usefull for  \n",
    "development purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ./course_ressources/udemy-docker-mastery/dockerfile-sample-2\n",
    "docker run -d -p 80:80 --name nginx -v $(pwd):/usr/share/nginx/html nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker run -d -p 8080:80 --name nginx2 nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding how mount points work\n",
    "\n",
    "If you go to localhost:80 you will see that the html being displayed is not the  \n",
    "default from nginx but the one from the directory that is being mapped to the  \n",
    "container. However if you go to localhost:8080 which was started without the mount  \n",
    "the you will see the default index.html being displayed. That is because if two files  \n",
    "(from host and from container) have the same name, the one from the host \"wins\" and  \n",
    "is the one used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51-52. Assignment: Database Upgrades with Named Volumes\n",
    "\n",
    "Updating the patch version of a postgres database. Outside of a container you  \n",
    "would upgrade the software however in containers you would not update what's  \n",
    "in a container but rather spin up a new container. So how should you proceed?  \n",
    "\n",
    "### Assignemnt Steps:\n",
    "\n",
    "1. Create a postgres container witha named volume psql-data using versio 9.6.1  \n",
    "2. Use docker hub to learn the volume path and the version needed to run it.  \n",
    "3. Check the logs on the startup of the firt container (admin user, default db)  \n",
    "and stop it.  \n",
    "4. Create a new postgress container with the same named volume using the newer  \n",
    "versio 9.6.2 (make sure the first container is stopped as you can't have two   \n",
    "containers using the same db)\n",
    "5. Check the logs and validate (you will see fewer startup log lines as the new  \n",
    "container doesn't have to repeat the same tasks that the first container has already  \n",
    "done)\n",
    "\n",
    "NOTE: this only works wit patch versions, most SQL DBs require manual commands   \n",
    "to upgrade DB to major/minor versions (its a DB limitation, not a container one).  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#check on existing volumes, you wont yet see psql-data volume\n",
    "docker volume ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run a postgres container of version 9.6.1-alpine.\n",
    "# Giving it a volume name that doesn't yet exist will get docker to create it\n",
    "# From the documentation you can see that the directory is used for access to volumes --> VOLUME [/var/lib/postgresql/data]\n",
    "# Link to image layers: https://hub.docker.com/layers/postgres/library/postgres/9.6.1-alpine/images/sha256-a47bdf2113e9a02e55899a26adebe3f5c64a3bd5c6e7e8abec39f18237607cf8?context=explore\n",
    "docker run -d -p 80:5432 --name postgres_9.6.1 -v psql-data:/var/lib/postgresql/data postgres:9.6.1-alpine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check the that volume was created\n",
    "docker volume inspect psql-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#inspec the image of postgres to see where it's configured for volumes\n",
    "docker image inspect postgres:9.6.1-alpine -f {{.ContainerConfig.Volumes}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#inspect the container to see to which volume it is mounted\n",
    "docker container inspect postgres_9.6.1 -f {{.Mounts}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# check the logs of the first container\n",
    "docker container logs postgres_9.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#stop the old container\n",
    "docker container stop postgres_9.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run -d -p 80:5432 --name postgres_9.6.2 -v psql-data:/var/lib/postgresql/data postgres:9.6.2-alpine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# You will see a lot less log lines because the database is already created, you're  \n",
    "# Just starting a new container on a later patch version to use that database where it is\n",
    "docker container logs postgres_9.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cleanup\n",
    "docker container stop postgres_9.6.2\n",
    "docker container rm -f postgres_9.6.1 postgres_9.6.2\n",
    "docker volume rm psql-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53.File Permissions Across Multiple Containers\n",
    "\n",
    "At some point you'll have file permission problems, maybe you want multiple   \n",
    "containers to access the same volume(s), or maybe you're bind mounting existing  \n",
    "files into a container.  \n",
    "\n",
    "NOTE: This is for pure LINUX hosts, if using Docker Desktop locally it will do   \n",
    "the translation of host permissions (macOS, Windows) into the containers (Linux).  \n",
    "\n",
    "\n",
    "File ownership between the host and container are just numbers (sometime you will  \n",
    "see the user friendly names but there is a one-to-one aliasing between those in  \n",
    "`/etc/passwd & /etc/group`). The host will have those files and your container will  \n",
    "also have its own set of these files. The linux kernel only care about these IDs.  \n",
    "\n",
    "Two separate problems can occur: \n",
    "\n",
    "1. The `/etc/passwd` is different across containers:  \n",
    "Creating a named user in one container and running it as user ID 700 but in another  \n",
    "container it has another user ID means the two containers have different `/etc/passwd`  \n",
    "files. Different names are fine as long as the IDs math, two processes trying to  \n",
    "access the same file must have a matching userID or groupID.\n",
    "\n",
    "2. Two containers are running as different users:  \n",
    "Maybe the user/group IDs and/or the USER statement in your Dockerfiles are   \n",
    "different, and the two containers are technically running under different IDs.  \n",
    "To troubleshoot this you:  \n",
    "    *  Use `ps aux` in each container to see a list of the processes/usernames  \n",
    "    that are running. The process needs a matchin userID or groupID to access the  \n",
    "    files  \n",
    "    * Find the UID/GID in each container's `/etc/passwd` and `/etc/group` to  \n",
    "    translate names to numbers, you'll likely find a mismatch between the container  \n",
    "    that wrote the file and the container that is trying to read it.  \n",
    "    * Find a way to sync up the UID/GID, this is usualy easier to do in your own  \n",
    "    custom images than in the official 3rd party images. This may mean creating a new  \n",
    "    user in the one Dockerfile and the startup user with [USER](https://docs.docker.com/engine/reference/builder/#user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54-55.Assignment: Edit Code Running In Containers With Bind Mounts\n",
    "\n",
    "You can edit files on host operating system and have them available in-sync inside  \n",
    "a running container. This is very cool for developement purposes as you can see changes  \n",
    "directly in the containers' behavior.\n",
    "\n",
    "### Assignment Steps:\n",
    "1. start a jekyll static website using the instructor's image in bretfisher/jekyll-serve\n",
    "2. make sure it is mounted to the working directory in which the assignment files   \n",
    "are present\n",
    "3. Update some of the files in the mounted directory and see them being reflected  \n",
    "in the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#start the container\n",
    "cd ./course_ressources/udemy-docker-mastery/bindmount-sample-1\n",
    "docker run -d -p 80:4000 -v $(pwd):/site --name jekyll-demo bretfisher/jekyll-serve \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# add some line to the markdown file that generates the static file\n",
    "# this change will be seem directly in the website as it hot reloads\n",
    "echo \"Here is some addtional line\" >> course_ressources/udemy-docker-mastery/bindmount-sample-1/_posts/2020-07-21-welcome-to-jekyll.markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# check the logs and see this in action \n",
    "docker container logs jekyll-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cleanup\n",
    "docker container rm -f jekyll-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56.Database Passwords in Containers\n",
    "\n",
    "When running postgres now, you'll need to either set a password, or tell it to   \n",
    "allow any connection. \n",
    "\n",
    "Passwords need to be set during `docker run` using environment variables  \n",
    "`POSTGRES_PASSWORD=mypasswd` or tell ti to ignore passowrds `POSTGRES_HOST_AUTH_METHOD=trust`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Making It Easier with Docker Compose: The multi-container tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 57.Docker Compose & The docker-compose.yml File\n",
    "\n",
    "Few software services are standalone, they would require other sevices to be running  \n",
    "(databases, front-end etc). How do  you make sure that the entire plant is up and running.\n",
    "\n",
    "There are two components:\n",
    "1. The docker-compose.yml file\n",
    "2. The docker-compose CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# A basic temaplate\n",
    "cat course_ressources/udemy-docker-mastery/compose-sample-1/template.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The docker-compose.yml file for the jekyll website that we ran in a previous\n",
    "# assignment\n",
    "cat course_ressources/udemy-docker-mastery/compose-sample-1/docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The docker-compose.yml file for a sample wordpress setup\n",
    "cat course_ressources/udemy-docker-mastery/compose-sample-1/compose-2.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The docker-compose.yml file for a 3 database cluster behind a ghost webserver\n",
    "cat course_ressources/udemy-docker-mastery/compose-sample-1/compose-3.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 58.Trying Out Basic Compose Commands\n",
    "\n",
    "CLI tool come with Windows/Mac but on Linux you'll need to download it. It is not a   \n",
    "production-grade tool but its ideal for loca dev and test.\n",
    "\n",
    "Two of its most common commands are:\n",
    "1. docker compose up # setup volumes/networks & start all containers\n",
    "2. docker compose down #stop all containers and remove containers/volumes/networks\n",
    "\n",
    "If you had a Dockerfile and a docker-compose.yml, the \"new develloper onboarding\"  \n",
    "would look like this:\n",
    "* git clone github.com/some_path\n",
    "* docker compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd course_ressources/udemy-docker-mastery/compose-sample-2/\n",
    "ls -ltra .\n",
    "\n",
    "printf '\\n\\ndocker-compose.yml\\n---------------------\\n'\n",
    "cat docker-compose.yml\n",
    "\n",
    "printf '\\n\\nnginx.conf\\n---------------------\\n'\n",
    "cat nginx.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd course_ressources/udemy-docker-mastery/compose-sample-2/\n",
    "docker-compose up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd course_ressources/udemy-docker-mastery/compose-sample-2/\n",
    "docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 59.Version Dependenciess in Multi-Tier Apps\n",
    "It's important to remember that every app with dependencies, will also have  \n",
    "version requirements for those dependencies.\n",
    "\n",
    "If you add an app and a database to a Compose file, then that app is going to  \n",
    "have specific database versions it is compatible with.\n",
    "\n",
    "Version dependencies aren't new, so they aren't technically a Docker thing, but  \n",
    "we *do* use Docker and Compose to control versions of apps like Drupal, PostgreSQL,  \n",
    "MySQL, Redis, Wordpress, etc.\n",
    "\n",
    "Therefore, when building your Dockerfile and docker-compose.yml file, remember  \n",
    "that you'll need to check the compatible versions in that apps documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60-61-62.Assignment: Build a Compose File for a Multi-Container Project\n",
    "\n",
    "Steps:\n",
    "* Build a basic Drupal content management server. It will need a database behind it.  \n",
    "Use postgres\n",
    "* Use port to expose Drupal to 8080 so you can localhost:8080\n",
    "* Be sure to set POSTGRES_PASSWORD\n",
    "* Walk through Drupal setup via browser\n",
    "* Tip: Drupal assumes the database is in localhost. S the Drupal Container will  \n",
    "assume its within the container (for a container localhost is itself, the container).  \n",
    "That is in conflict with the principle of one container, one service (use the service name  \n",
    "instead for DNS resolution)\n",
    "* Extra credit: Use volumes to store Drupal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd course_ressources/udemy-docker-mastery/compose-assignment-1\n",
    "ls -trla\n",
    "mkdir assignment-submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer\n",
    "```\n",
    "version: '2'\n",
    "\n",
    "services:\n",
    "  drupal:\n",
    "    image: drupal:8.8.2\n",
    "    ports:\n",
    "      - \"8080:80\"\n",
    "    volumes:\n",
    "      - drupal-modules:/var/www/html/modules\n",
    "      - drupal-profiles:/var/www/html/profiles       \n",
    "      - drupal-sites:/var/www/html/sites      \n",
    "      - drupal-themes:/var/www/html/themes\n",
    "  postgres:\n",
    "    image: postgres:12.1\n",
    "    environment:\n",
    "      - POSTGRES_PASSWORD=mypasswd\n",
    "\n",
    "volumes:\n",
    "  drupal-modules:\n",
    "  drupal-profiles:\n",
    "  drupal-sites:\n",
    "  drupal-themes:\n",
    "\n",
    "        \n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd course_ressources/udemy-docker-mastery/compose-assignment-1/answer\n",
    "docker compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd course_ressources/udemy-docker-mastery/compose-assignment-1/answer\n",
    "docker compose down -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 63- Adding image building into compose files\n",
    "\n",
    "Docker compose will build images they're not found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "-rw-r--r--   1 karimitani  staff   347 Aug 18 18:48 docker-compose.yml\n",
      "drwxr-xr-x  14 karimitani  staff   448 Aug 18 18:48 html\n",
      "-rw-r--r--   1 karimitani  staff    64 Aug 18 18:48 nginx.Dockerfile\n",
      "drwxr-xr-x   6 karimitani  staff   192 Aug 18 18:48 .\n",
      "-rw-r--r--   1 karimitani  staff   298 Aug 18 18:48 nginx.conf\n",
      "drwxr-xr-x  36 karimitani  staff  1152 Aug 18 18:48 ..\n",
      "version: '2'\n",
      "\n",
      "# based off compose-sample-2, only we build nginx.conf into image\n",
      "# uses sample HTML static site from https://startbootstrap.com/themes/agency/\n",
      "\n",
      "services:\n",
      "  proxy:\n",
      "    build:\n",
      "      context: .\n",
      "      dockerfile: nginx.Dockerfile\n",
      "    ports:\n",
      "      - '80:80'\n",
      "  web:\n",
      "    image: httpd\n",
      "    volumes:\n",
      "      - ./html:/usr/local/apache2/htdocs/\n",
      "FROM nginx:1.13\n",
      "\n",
      "COPY nginx.conf /etc/nginx/conf.d/default.conf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd  ./course_ressources/udemy-docker-mastery/compose-sample-3\n",
    "ls -ltra\n",
    "\n",
    "# You can see that you specify dockerfile for the nginx image and you're linking\n",
    "# the html directory into the Apache container's directory\n",
    "cat docker-compose.yml\n",
    "\n",
    "cat nginx.Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64-65.Assignment : Build and Run Compose\n",
    "\n",
    "Building a custom drupal image for local testing.\n",
    "\n",
    "Assignment directions:\n",
    "* Start with the compose file from the last assignment\n",
    "* Make you Dockerfile and docker-compose.yl in dir compose-assignemt-2\n",
    "* Use the README.md for details\n",
    "\n",
    "When you have a build and image key in the compose file you change the purpose of \n",
    "the image command. You're not pulling the image with that name form the registry  \n",
    "but are actually building from the directory specified inthe build/context field  \n",
    "and name that image with the name specified in the image field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission:\n",
    "\n",
    "\n",
    "```\n",
    "Dockerfile\n",
    "FROM drupal:8.2\n",
    "\n",
    "#RUN apt package manager command to install git\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "\n",
    "\n",
    "WORKDIR /var/www/html/themes\n",
    "\n",
    "#use git to clone the theme\n",
    "RUN git clone --branch 8.x-3.x --single-branch --depth 1 https://git.drupalcode.org/project/bootstrap.git \\\n",
    "    #we need to change permissions on files\n",
    "    && chown -R www-data:www-data bootstrap \n",
    "\n",
    "WORKDIR /var/www/html\n",
    "FROM drupal:8.2\n",
    "\n",
    "#RUN apt package manager command to install git\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "\n",
    "\n",
    "WORKDIR /var/www/html/themes\n",
    "\n",
    "#use git to clone the theme\n",
    "RUN git clone --branch 8.x-3.x --single-branch --depth 1 https://git.drupalcode.org/project/bootstrap.git \\\n",
    "    #we need to change permissions on files\n",
    "    && chown -R www-data:www-data bootstrap \n",
    "\n",
    "WORKDIR /var/www/html\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "docker-compose.yml\n",
    "version: '2'\n",
    "\n",
    "services:\n",
    "  drupal:\n",
    "    image: custom-drupal\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"8080:80\"\n",
    "    volumes:\n",
    "      - drupal-modules:/var/www/html/modules\n",
    "      - drupal-profiles:/var/www/html/profiles\n",
    "      - drupal-sites:/var/www/html/sites\n",
    "      - drupal-themes:/var/www/html/themes\n",
    "\n",
    "  postgres:\n",
    "    image: postgres:12.1\n",
    "    environment:\n",
    "      - POSTGRES_PASSWORD=mypasswd\n",
    "    volumes:\n",
    "      - drupal-data:/var/lib/postgresql/data\n",
    "volumes:\n",
    "  drupal-modules:\n",
    "  drupal-profiles:\n",
    "  drupal-sites:\n",
    "  drupal-themes:\n",
    "  drupal-data:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "-rw-r--r--   1 karimitani  staff  3619 Aug 18 18:48 README.md\n",
      "drwxr-xr-x   4 karimitani  staff   128 Aug 18 18:48 answer\n",
      "drwxr-xr-x   6 karimitani  staff   192 Aug 18 18:48 .\n",
      "drwxr-xr-x  36 karimitani  staff  1152 Aug 18 18:48 ..\n",
      "-rw-r--r--   1 karimitani  staff   434 Sep 13 04:08 Dockerfile\n",
      "-rw-r--r--   1 karimitani  staff   576 Sep 13 04:16 docker-compose.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating network \"compose-assignment-2_default\" with the default driver\n",
      "Creating volume \"compose-assignment-2_drupal-modules\" with default driver\n",
      "Creating volume \"compose-assignment-2_drupal-profiles\" with default driver\n",
      "Creating volume \"compose-assignment-2_drupal-sites\" with default driver\n",
      "Creating volume \"compose-assignment-2_drupal-themes\" with default driver\n",
      "Creating volume \"compose-assignment-2_drupal-data\" with default driver\n",
      "Building drupal\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:bb29ab4b8b67eef3785c7934ae48de63903be38b4d938d8c39f9791d93786781\n",
      "#1 transferring dockerfile:\n",
      "#1 transferring dockerfile: 478B 1.1s done\n",
      "#1 DONE 1.2s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:1f95ca88be94589e82719ee2aec8bb8fc8dbb5be2d2142fd826a4b3005d4adcd\n",
      "#2 transferring context:\n",
      "#2 transferring context: 2B 0.0s done\n",
      "#2 DONE 0.3s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/drupal:8.2\n",
      "#3 sha256:98e346af38bc52429da738db4b04bbf3643a0f0d6f6b8e9090f966c75f00fe9c\n",
      "#3 DONE 4.6s\n",
      "\n",
      "#4 [1/5] FROM docker.io/library/drupal:8.2@sha256:f2dc3f325da9c9e017f5e2ee0e22a72781cdded21955e533c412d35e09304f8c\n",
      "#4 sha256:d60ea73d155e0611b529514e4ced91b601cf672ca8179e9fc9db5e65ef256b0c\n",
      "#4 resolve docker.io/library/drupal:8.2@sha256:f2dc3f325da9c9e017f5e2ee0e22a72781cdded21955e533c412d35e09304f8c 0.0s done\n",
      "#4 sha256:68b9d32702eeaed0836dc2aafc0fe39419569d68e5b31b92878aa00858309314 14.35kB / 14.35kB done\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 0B / 82.49MB 0.2s\n",
      "#4 sha256:f2dc3f325da9c9e017f5e2ee0e22a72781cdded21955e533c412d35e09304f8c 3.87kB / 3.87kB done\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 0B / 52.57MB 0.2s\n",
      "#4 sha256:c80a1f6ddd8d3ad32ed2207ec2ddba3002af112fdc44df8a268644731ccbef42 0B / 178B 0.2s\n",
      "#4 sha256:c80a1f6ddd8d3ad32ed2207ec2ddba3002af112fdc44df8a268644731ccbef42 178B / 178B 1.2s done\n",
      "#4 sha256:c3e9b0c1871f3a98d9c059e5bd8c32329922b597e955aeb2996b83611a31ed1c 0B / 2.78MB 1.4s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 3.15MB / 52.57MB 2.0s\n",
      "#4 sha256:c3e9b0c1871f3a98d9c059e5bd8c32329922b597e955aeb2996b83611a31ed1c 1.05MB / 2.78MB 2.5s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 6.29MB / 52.57MB 2.8s\n",
      "#4 sha256:c3e9b0c1871f3a98d9c059e5bd8c32329922b597e955aeb2996b83611a31ed1c 2.10MB / 2.78MB 2.8s\n",
      "#4 sha256:c3e9b0c1871f3a98d9c059e5bd8c32329922b597e955aeb2996b83611a31ed1c 2.78MB / 2.78MB 2.9s\n",
      "#4 sha256:c3e9b0c1871f3a98d9c059e5bd8c32329922b597e955aeb2996b83611a31ed1c 2.78MB / 2.78MB 2.9s done\n",
      "#4 sha256:b5e2756f5f08ec966dbb623239d79701ffad4658c720373b154ec326f2458532 0B / 1.25kB 3.0s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 9.44MB / 52.57MB 3.7s\n",
      "#4 sha256:b5e2756f5f08ec966dbb623239d79701ffad4658c720373b154ec326f2458532 1.25kB / 1.25kB 3.5s done\n",
      "#4 sha256:cd2ca1686b1712d41666865c88a26db11ecb736950c4a8a4070198c1e1b77951 0B / 426B 3.7s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 5.24MB / 82.49MB 4.2s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 12.58MB / 52.57MB 4.2s\n",
      "#4 sha256:cd2ca1686b1712d41666865c88a26db11ecb736950c4a8a4070198c1e1b77951 426B / 426B 4.1s done\n",
      "#4 sha256:529b20a8291dfb15667d29b65693a605462198881fca7d83d77e68c985457388 0B / 221B 4.2s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 15.73MB / 52.57MB 5.0s\n",
      "#4 sha256:529b20a8291dfb15667d29b65693a605462198881fca7d83d77e68c985457388 221B / 221B 4.8s done\n",
      "#4 sha256:dd0c35492a16533f7fd075deaabee62790d1b0a914df163d74f7d6bc140fe256 0B / 475B 5.0s\n",
      "#4 sha256:dd0c35492a16533f7fd075deaabee62790d1b0a914df163d74f7d6bc140fe256 475B / 475B 6.4s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 9.44MB / 82.49MB 6.7s\n",
      "#4 sha256:dd0c35492a16533f7fd075deaabee62790d1b0a914df163d74f7d6bc140fe256 475B / 475B 6.4s done\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 0B / 12.93MB 7.0s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 18.87MB / 52.57MB 7.4s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 1.05MB / 12.93MB 8.4s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 13.63MB / 82.49MB 8.5s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 22.02MB / 52.57MB 8.8s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 2.10MB / 12.93MB 8.8s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 3.15MB / 12.93MB 9.2s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 4.19MB / 12.93MB 9.6s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 5.24MB / 12.93MB 9.8s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 25.17MB / 52.57MB 10.3s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 6.29MB / 12.93MB 10.3s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 17.83MB / 82.49MB 10.4s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 7.34MB / 12.93MB 11.0s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 8.39MB / 12.93MB 11.1s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 28.31MB / 52.57MB 11.4s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 9.44MB / 12.93MB 11.4s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 22.02MB / 82.49MB 11.6s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 10.49MB / 12.93MB 11.6s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 31.46MB / 52.57MB 12.0s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 11.53MB / 12.93MB 12.0s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 12.93MB / 12.93MB 12.2s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 26.21MB / 82.49MB 12.5s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 34.60MB / 52.57MB 12.7s\n",
      "#4 sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 12.93MB / 12.93MB 12.5s done\n",
      "#4 sha256:47d132d3206bc994878eb81bcdc980b8d21fabdb66f9ef517ee7ac115818f6e2 0B / 497B 12.7s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 30.41MB / 82.49MB 13.1s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 37.75MB / 52.57MB 13.1s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 34.95MB / 82.49MB 13.6s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 41.94MB / 52.57MB 13.6s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 39.63MB / 82.49MB 14.1s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 45.09MB / 52.57MB 14.1s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 48.23MB / 52.57MB 14.5s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 44.04MB / 82.49MB 14.7s\n",
      "#4 sha256:47d132d3206bc994878eb81bcdc980b8d21fabdb66f9ef517ee7ac115818f6e2 497B / 497B 14.5s done\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 0B / 13.52MB 14.7s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 51.38MB / 52.57MB 14.9s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 48.23MB / 82.49MB 15.1s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 52.43MB / 82.49MB 15.4s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 56.62MB / 82.49MB 15.8s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 61.87MB / 82.49MB 16.3s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 1.05MB / 13.52MB 16.6s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 66.06MB / 82.49MB 16.9s\n",
      "#4 sha256:de4fe1fce8f952592eed2a1a4155bc91821e0c33f4da3e54d3e3e5a78356e011 0B / 2.13kB 17.3s\n",
      "#4 sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 52.57MB / 52.57MB 17.2s done\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 1.96MB / 13.52MB 17.4s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 70.25MB / 82.49MB 17.5s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 75.50MB / 82.49MB 17.8s\n",
      "#4 sha256:de4fe1fce8f952592eed2a1a4155bc91821e0c33f4da3e54d3e3e5a78356e011 2.13kB / 2.13kB 17.8s done\n",
      "#4 sha256:c99eafb09b5521dcf438995e32a11a38a7b8edeeefc9817e0ebd16ff1a0e97f5 0B / 893B 18.0s\n",
      "#4 extracting sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 82.49MB / 82.49MB 18.3s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 3.15MB / 13.52MB 18.3s\n",
      "#4 sha256:c99eafb09b5521dcf438995e32a11a38a7b8edeeefc9817e0ebd16ff1a0e97f5 893B / 893B 18.5s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 4.19MB / 13.52MB 19.0s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 5.24MB / 13.52MB 20.0s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 6.29MB / 13.52MB 20.5s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 7.34MB / 13.52MB 20.8s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 8.39MB / 13.52MB 21.0s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 11.52MB / 13.52MB 21.5s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 13.52MB / 13.52MB 21.8s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 82.49MB / 82.49MB 23.6s\n",
      "#4 sha256:c99eafb09b5521dcf438995e32a11a38a7b8edeeefc9817e0ebd16ff1a0e97f5 893B / 893B 23.6s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 13.52MB / 13.52MB 27.0s\n",
      "#4 extracting sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 9.4s\n",
      "#4 sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 82.49MB / 82.49MB 27.8s done\n",
      "#4 sha256:c99eafb09b5521dcf438995e32a11a38a7b8edeeefc9817e0ebd16ff1a0e97f5 893B / 893B 27.8s done\n",
      "#4 sha256:fdfd1196d5778d037deaefd242ef49011f3aa42c4ec81ad1ab17901ac825545f 0B / 291B 27.9s\n",
      "#4 sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 13.52MB / 13.52MB 28.9s done\n",
      "#4 sha256:0471f45260835db3eff553630e6ac8f80db6c43a42ac5e503a2c35ba185c9cce 0B / 334B 29.1s\n",
      "#4 sha256:4b7e97ab419a7bcd77498302bac2806573db24ae941756d3a695393379a88d9f 0B / 1.88MB 29.3s\n",
      "#4 sha256:fdfd1196d5778d037deaefd242ef49011f3aa42c4ec81ad1ab17901ac825545f 291B / 291B 30.2s done\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 0B / 12.19MB 30.2s\n",
      "#4 sha256:0471f45260835db3eff553630e6ac8f80db6c43a42ac5e503a2c35ba185c9cce 334B / 334B 30.6s\n",
      "#4 sha256:0471f45260835db3eff553630e6ac8f80db6c43a42ac5e503a2c35ba185c9cce 334B / 334B 30.6s done\n",
      "#4 sha256:4b7e97ab419a7bcd77498302bac2806573db24ae941756d3a695393379a88d9f 1.05MB / 1.88MB 30.7s\n",
      "#4 sha256:4b7e97ab419a7bcd77498302bac2806573db24ae941756d3a695393379a88d9f 1.88MB / 1.88MB 30.9s\n",
      "#4 sha256:4b7e97ab419a7bcd77498302bac2806573db24ae941756d3a695393379a88d9f 1.88MB / 1.88MB 31.1s done\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 1.05MB / 12.19MB 31.9s\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 2.77MB / 12.19MB 32.2s\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 4.52MB / 12.19MB 32.5s\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 5.24MB / 12.19MB 32.7s\n",
      "#4 extracting sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 14.6s\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 8.39MB / 12.19MB 32.9s\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 9.44MB / 12.19MB 33.0s\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 12.19MB / 12.19MB 33.2s\n",
      "#4 sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 12.19MB / 12.19MB 33.8s done\n",
      "#4 extracting sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 26.8s\n",
      "#4 extracting sha256:ef0380f84d05d3cdc5a5f660232d35ccb020ccf1d635b585580dea44691a13a7 28.8s done\n",
      "#4 extracting sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688\n",
      "#4 extracting sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 5.2s\n",
      "#4 extracting sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 10.3s\n",
      "#4 extracting sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 15.3s\n",
      "#4 extracting sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 20.5s\n",
      "#4 extracting sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 25.9s\n",
      "#4 extracting sha256:d676534ff31563c0d3cd60a30137d929e3731fcff3702b1dab1690cf60fc6688 29.6s done\n",
      "#4 extracting sha256:c80a1f6ddd8d3ad32ed2207ec2ddba3002af112fdc44df8a268644731ccbef42\n",
      "#4 extracting sha256:c80a1f6ddd8d3ad32ed2207ec2ddba3002af112fdc44df8a268644731ccbef42 done\n",
      "#4 extracting sha256:c3e9b0c1871f3a98d9c059e5bd8c32329922b597e955aeb2996b83611a31ed1c\n",
      "#4 extracting sha256:c3e9b0c1871f3a98d9c059e5bd8c32329922b597e955aeb2996b83611a31ed1c 2.2s done\n",
      "#4 extracting sha256:b5e2756f5f08ec966dbb623239d79701ffad4658c720373b154ec326f2458532\n",
      "#4 extracting sha256:b5e2756f5f08ec966dbb623239d79701ffad4658c720373b154ec326f2458532 done\n",
      "#4 extracting sha256:cd2ca1686b1712d41666865c88a26db11ecb736950c4a8a4070198c1e1b77951\n",
      "#4 extracting sha256:cd2ca1686b1712d41666865c88a26db11ecb736950c4a8a4070198c1e1b77951 done\n",
      "#4 extracting sha256:529b20a8291dfb15667d29b65693a605462198881fca7d83d77e68c985457388\n",
      "#4 extracting sha256:529b20a8291dfb15667d29b65693a605462198881fca7d83d77e68c985457388 done\n",
      "#4 extracting sha256:dd0c35492a16533f7fd075deaabee62790d1b0a914df163d74f7d6bc140fe256\n",
      "#4 extracting sha256:dd0c35492a16533f7fd075deaabee62790d1b0a914df163d74f7d6bc140fe256 done\n",
      "#4 extracting sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347\n",
      "#4 extracting sha256:7b816d914b8a222a0ce6a116e3667f5a0daa258bbc69fef8e374ffd0d2263347 0.6s done\n",
      "#4 extracting sha256:47d132d3206bc994878eb81bcdc980b8d21fabdb66f9ef517ee7ac115818f6e2\n",
      "#4 extracting sha256:47d132d3206bc994878eb81bcdc980b8d21fabdb66f9ef517ee7ac115818f6e2 0.0s done\n",
      "#4 extracting sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1\n",
      "#4 extracting sha256:3e09aa92a921d6c91cc7b673476e05adc1cfcd5b3e2030b0973a1dbfe43cfec1 6.0s done\n",
      "#4 extracting sha256:de4fe1fce8f952592eed2a1a4155bc91821e0c33f4da3e54d3e3e5a78356e011 0.0s done\n",
      "#4 extracting sha256:c99eafb09b5521dcf438995e32a11a38a7b8edeeefc9817e0ebd16ff1a0e97f5 done\n",
      "#4 extracting sha256:fdfd1196d5778d037deaefd242ef49011f3aa42c4ec81ad1ab17901ac825545f\n",
      "#4 extracting sha256:fdfd1196d5778d037deaefd242ef49011f3aa42c4ec81ad1ab17901ac825545f 0.0s done\n",
      "#4 extracting sha256:4b7e97ab419a7bcd77498302bac2806573db24ae941756d3a695393379a88d9f\n",
      "#4 extracting sha256:4b7e97ab419a7bcd77498302bac2806573db24ae941756d3a695393379a88d9f 0.5s done\n",
      "#4 extracting sha256:0471f45260835db3eff553630e6ac8f80db6c43a42ac5e503a2c35ba185c9cce 0.0s done\n",
      "#4 extracting sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a\n",
      "#4 extracting sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 5.0s\n",
      "#4 extracting sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 10.1s\n",
      "#4 extracting sha256:e3eef4d3da95102bd1d8fff4221cf7038bba3f3ea6e5843706bd8ff18042127a 10.8s done\n",
      "#4 DONE 106.4s\n",
      "\n",
      "#5 [2/5] RUN apt-get update     && apt-get install -y git     && rm -rf /var/lib/apt/lists/*\n",
      "#5 sha256:b58dbbc4fb02aeddd0ada75bfe5ed83ebd70b1d3aad5bdf6b64ccafffa09fb68\n",
      "#5 20.53 Ign http://deb.debian.org jessie InRelease\n",
      "#5 20.53 Get:1 http://security.debian.org jessie/updates InRelease [44.9 kB]\n",
      "#5 24.79 Get:2 http://deb.debian.org jessie-updates InRelease [16.3 kB]\n",
      "#5 24.98 Get:3 http://deb.debian.org jessie Release.gpg [1652 B]\n",
      "#5 25.61 Get:4 http://deb.debian.org jessie Release [77.3 kB]\n",
      "#5 26.40 Get:5 http://security.debian.org jessie/updates/main amd64 Packages [992 kB]\n",
      "#5 37.62 Get:6 http://deb.debian.org jessie-updates/main amd64 Packages [20 B]\n",
      "#5 37.76 Get:7 http://deb.debian.org jessie/main amd64 Packages [9098 kB]\n",
      "#5 45.64 Fetched 10.2 MB in 28s (362 kB/s)\n",
      "#5 45.64 Reading package lists...\n",
      "#5 49.76 W: There is no public key available for the following key IDs:\n",
      "#5 49.76 AA8E81B4331F7F50\n",
      "#5 50.69 Reading package lists...\n",
      "#5 54.98 Building dependency tree...\n",
      "#5 55.17 Reading state information...\n",
      "#5 55.45 The following extra packages will be installed:\n",
      "#5 55.45   git-man less libcurl3-gnutls liberror-perl libpopt0 libx11-6 libx11-data\n",
      "#5 55.45   libxau6 libxcb1 libxdmcp6 libxext6 libxmuu1 openssh-client rsync xauth\n",
      "#5 55.45 Suggested packages:\n",
      "#5 55.45   gettext-base git-daemon-run git-daemon-sysvinit git-doc git-el git-email\n",
      "#5 55.45   git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn ssh-askpass\n",
      "#5 55.45   libpam-ssh keychain monkeysphere openssh-server\n",
      "#5 55.45 Recommended packages:\n",
      "#5 55.45   ssh-client\n",
      "#5 55.46 The following NEW packages will be installed:\n",
      "#5 55.46   git git-man less libcurl3-gnutls liberror-perl libpopt0 libx11-6 libx11-data\n",
      "#5 55.46   libxau6 libxcb1 libxdmcp6 libxext6 libxmuu1 openssh-client rsync xauth\n",
      "#5 58.62 0 upgraded, 16 newly installed, 0 to remove and 94 not upgraded.\n",
      "#5 58.62 Need to get 7092 kB of archives.\n",
      "#5 58.62 After this operation, 33.3 MB of additional disk space will be used.\n",
      "#5 58.62 Get:1 http://security.debian.org/ jessie/updates/main libcurl3-gnutls amd64 7.38.0-4+deb8u16 [253 kB]\n",
      "#5 58.77 Get:2 http://security.debian.org/ jessie/updates/main libxdmcp6 amd64 1:1.1.1-1+deb8u1 [24.7 kB]\n",
      "#5 58.89 Get:3 http://deb.debian.org/debian/ jessie/main libpopt0 amd64 1.16-10 [49.2 kB]\n",
      "#5 59.01 Get:4 http://deb.debian.org/debian/ jessie/main libxau6 amd64 1:1.0.8-1 [20.7 kB]\n",
      "#5 59.06 Get:5 http://security.debian.org/ jessie/updates/main libx11-data all 2:1.6.2-3+deb8u2 [126 kB]\n",
      "#5 59.22 Get:6 http://deb.debian.org/debian/ jessie/main libxcb1 amd64 1.10-3+b1 [44.4 kB]\n",
      "#5 59.29 Get:7 http://security.debian.org/ jessie/updates/main libx11-6 amd64 2:1.6.2-3+deb8u2 [729 kB]\n",
      "#5 59.46 Get:8 http://security.debian.org/ jessie/updates/main openssh-client amd64 1:6.7p1-5+deb8u8 [696 kB]\n",
      "#5 59.50 Get:9 http://deb.debian.org/debian/ jessie/main libxext6 amd64 2:1.3.3-1 [52.7 kB]\n",
      "#5 59.65 Get:10 http://security.debian.org/ jessie/updates/main git-man all 1:2.1.4-2.1+deb8u10 [1271 kB]\n",
      "#5 59.69 Get:11 http://deb.debian.org/debian/ jessie/main libxmuu1 amd64 2:1.1.2-1 [23.3 kB]\n",
      "#5 59.77 Get:12 http://deb.debian.org/debian/ jessie/main less amd64 458-3 [124 kB]\n",
      "#5 59.94 Get:13 http://deb.debian.org/debian/ jessie/main liberror-perl all 0.17-1.1 [22.4 kB]\n",
      "#5 59.99 Get:14 http://security.debian.org/ jessie/updates/main git amd64 1:2.1.4-2.1+deb8u10 [3227 kB]\n",
      "#5 60.09 Get:15 http://deb.debian.org/debian/ jessie/main xauth amd64 1:1.0.9-1 [38.2 kB]\n",
      "#5 60.35 Get:16 http://security.debian.org/ jessie/updates/main rsync amd64 3.1.1-3+deb8u2 [390 kB]\n",
      "#5 62.15 debconf: delaying package configuration, since apt-utils is not installed\n",
      "#5 62.16 Fetched 7092 kB in 1s (3802 kB/s)\n",
      "#5 65.81 dpkg: could not open log '/var/log/dpkg.log': Input/output error\n",
      "#5 65.92 dpkg: error processing archive /var/cache/apt/archives/libpopt0_1.16-10_amd64.deb (--unpack):\n",
      "#5 65.92  unable to sync file '/var/lib/dpkg/tmp.ci//symbols': Input/output error\n",
      "#5 66.05 dpkg: error processing archive /var/cache/apt/archives/libcurl3-gnutls_7.38.0-4+deb8u16_amd64.deb (--unpack):\n",
      "#5 66.05  unable to sync file '/var/lib/dpkg/tmp.ci//symbols': Input/output error\n",
      "#5 66.55 dpkg: error processing archive /var/cache/apt/archives/libxau6_1%3a1.0.8-1_amd64.deb (--unpack):\n",
      "#5 66.55  unable to sync file '/var/lib/dpkg/tmp.ci//postrm': Input/output error\n",
      "#5 66.82 dpkg: error processing archive /var/cache/apt/archives/libxdmcp6_1%3a1.1.1-1+deb8u1_amd64.deb (--unpack):\n",
      "#5 66.82  unable to sync file '/var/lib/dpkg/tmp.ci//md5sums': Input/output error\n",
      "#5 66.88 dpkg: error processing archive /var/cache/apt/archives/libxcb1_1.10-3+b1_amd64.deb (--unpack):\n",
      "#5 66.88  unable to sync file '/var/lib/dpkg/tmp.ci//symbols': Input/output error\n",
      "#5 67.03 dpkg: error processing archive /var/cache/apt/archives/libx11-data_2%3a1.6.2-3+deb8u2_all.deb (--unpack):\n",
      "#5 67.03  unable to sync file '/var/lib/dpkg/tmp.ci//md5sums': Input/output error\n",
      "#5 67.24 dpkg: error processing archive /var/cache/apt/archives/libx11-6_2%3a1.6.2-3+deb8u2_amd64.deb (--unpack):\n",
      "#5 67.24  unable to sync file '/var/lib/dpkg/tmp.ci//symbols': Input/output error\n",
      "#5 67.53 dpkg: error processing archive /var/cache/apt/archives/libxext6_2%3a1.3.3-1_amd64.deb (--unpack):\n",
      "#5 67.53  unable to sync file '/var/lib/dpkg/tmp.ci//shlibs': Input/output error\n",
      "#5 67.70 dpkg: error processing archive /var/cache/apt/archives/libxmuu1_2%3a1.1.2-1_amd64.deb (--unpack):\n",
      "#5 67.70  unable to sync file '/var/lib/dpkg/tmp.ci//shlibs': Input/output error\n",
      "#5 67.80 dpkg: error processing archive /var/cache/apt/archives/less_458-3_amd64.deb (--unpack):\n",
      "#5 67.80  unable to sync file '/var/lib/dpkg/tmp.ci//prerm': Input/output error\n",
      "#5 67.84 dpkg: error processing archive /var/cache/apt/archives/openssh-client_1%3a6.7p1-5+deb8u8_amd64.deb (--unpack):\n",
      "#5 67.84  unable to sync file '/var/lib/dpkg/tmp.ci//prerm': Input/output error\n",
      "#5 68.20 dpkg: error processing archive /var/cache/apt/archives/liberror-perl_0.17-1.1_all.deb (--unpack):\n",
      "#5 68.20  unable to sync file '/var/lib/dpkg/tmp.ci//md5sums': Input/output error\n",
      "#5 68.46 dpkg: error processing archive /var/cache/apt/archives/git-man_1%3a2.1.4-2.1+deb8u10_all.deb (--unpack):\n",
      "#5 68.46  unable to sync file '/var/lib/dpkg/tmp.ci//md5sums': Input/output error\n",
      "#5 68.58 dpkg: error processing archive /var/cache/apt/archives/git_1%3a2.1.4-2.1+deb8u10_amd64.deb (--unpack):\n",
      "#5 68.58  unable to sync file '/var/lib/dpkg/tmp.ci//preinst': Input/output error\n",
      "#5 68.67 dpkg: error processing archive /var/cache/apt/archives/rsync_3.1.1-3+deb8u2_amd64.deb (--unpack):\n",
      "#5 68.67  unable to sync file '/var/lib/dpkg/tmp.ci//prerm': Input/output error\n",
      "#5 69.01 dpkg: error processing archive /var/cache/apt/archives/xauth_1%3a1.0.9-1_amd64.deb (--unpack):\n",
      "#5 69.01  unable to sync file '/var/lib/dpkg/tmp.ci//md5sums': Input/output error\n",
      "#5 69.10 dpkg: error: unable to sync new file '/var/lib/dpkg/status-new': Input/output error\n",
      "#5 73.84 W: Could not open file '/var/log/apt/term.log' - OpenLog (5: Input/output error)\n",
      "#5 73.84 E: Sub-process /usr/bin/dpkg returned an error code (2)\n",
      "#5 ERROR: executor failed running [/bin/sh -c apt-get update     && apt-get install -y git     && rm -rf /var/lib/apt/lists/*]: exit code: 100\n",
      "------\n",
      " > [2/5] RUN apt-get update     && apt-get install -y git     && rm -rf /var/lib/apt/lists/*:\n",
      "------\n",
      "executor failed running [/bin/sh -c apt-get update     && apt-get install -y git     && rm -rf /var/lib/apt/lists/*]: exit code: 100\n",
      "Service 'drupal' failed to build : Build failed\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd  ./course_ressources/udemy-docker-mastery/compose-assignment-2\\nls -ltra\\ndocker-compose up \\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/karimitani/Documents/github/Docker_Mastery/course_notes.ipynb Cell 200\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karimitani/Documents/github/Docker_Mastery/course_notes.ipynb#Y411sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mbash\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcd  ./course_ressources/udemy-docker-mastery/compose-assignment-2\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mls -ltra\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mdocker-compose up \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py:2358\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2356\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2357\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2358\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2359\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'cd  ./course_ressources/udemy-docker-mastery/compose-assignment-2\\nls -ltra\\ndocker-compose up \\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd  ./course_ressources/udemy-docker-mastery/compose-assignment-2\n",
    "ls -ltra\n",
    "docker-compose up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
